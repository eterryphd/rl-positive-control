# Requirements for GRPO training with Llama 3.1 8B on 4x A40
# 
# NOTE: TRL ecosystem moves fast. These ranges worked as of Dec 2024.
# If you hit issues, fall back to exact pinned versions below.
#
# PINNED FALLBACK (known working combination):
#   pip install torch==2.4.0 transformers==4.48.1 trl==0.18.0 \
#       accelerate==1.3.0 deepspeed==0.15.4 vllm==0.7.0

# Core ML
torch>=2.4.0
transformers>=4.48.0,<4.54.0

# TRL and RL training - need 0.18+ for vllm_mode parameter
trl>=0.18.0

# Distributed training
accelerate>=1.3.0,<1.5.0
deepspeed>=0.15.0,<0.17.0

# vLLM for fast generation during GRPO rollouts
vllm>=0.7.0

# Dataset handling
datasets>=3.0.0

# Utilities
huggingface-hub
tqdm