# Requirements for GRPO training with Llama 3.1 8B on 4x A40
# 
# TESTED WORKING STACK (from HuggingFace Cookbook, Dec 2025):
#   trl==0.23.1 + vllm==0.11.0 + transformers==4.57.0
#
# TRL only supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2

# Core ML
torch>=2.5.0
transformers==4.57.0

# TRL - need 0.23+ for stable vllm_mode="server"
trl==0.23.1

# Distributed training
accelerate>=1.4.0
deepspeed>=0.16.0

# vLLM - MUST be in supported range
vllm==0.11.0

# Dataset handling
datasets>=3.0.0

# Utilities
huggingface-hub
tqdm